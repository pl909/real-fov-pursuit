pybullet build time: Jan 29 2025 23:16:28
/home/pl217/miniconda3/envs/drones/lib/python3.10/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
/home/pl217/miniconda3/envs/drones/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
/home/pl217/miniconda3/envs/drones/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Traceback (most recent call last):
  File "/home/pl217/gym-pybullet-drones/gym_pybullet_drones/examples/learn_pursuit.py", line 313, in <module>
    run(**vars(ARGS))
  File "/home/pl217/gym-pybullet-drones/gym_pybullet_drones/examples/learn_pursuit.py", line 268, in run
    logger.log(drone=d,
  File "/zpool-00/home/pl217/gym-pybullet-drones/gym_pybullet_drones/utils/Logger.py", line 117, in log
    self.states[drone, :, current_counter] = np.hstack([state[0:3], state[10:13], state[7:10], state[13:20]])
ValueError: could not broadcast input array from shape (15,) into shape (16,)
